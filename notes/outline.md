# Agentic Coding Tools – Presentation Outline

## What is Agent Mode?

- Definition: An AI coding assistant mode where you describe a high-level task and the AI autonomously plans and applies the needed code changes across your project ￼. It’s more proactive than simple autocomplete.
- How it works: Think of it as assigning a task to a junior developer: the agent figures out which files/functions to modify and even runs tools (like tests or build commands) to get the job done ￼.
- Use cases: Great for larger or vague tasks that span multiple files. For example, refactoring the codebase (“use Redis for caching”), adding a new feature (like an OAuth login), or migrating to a different framework ￼. The agent will handle planning and editing in all relevant places.

## How to Access Agent Mode in VS Code

- Prerequisites: Ensure you have the latest VS Code (v1.99 or later) and the GitHub Copilot extension with chat enabled ￼. (Agent mode is part of Copilot Chat, so you need Copilot access.)
- Enabling it: In VS Code settings, turn on the “Copilot: Chat Agent” feature (setting name chat.agent.enabled) ￼. This might be on by default if you’re up-to-date, but double-check.
- Using it: Open the Copilot Chat panel in VS Code and switch the chat mode to Agent (instead of the usual Ask or Edit modes) ￼. Then you can type your request. The agent will likely ask for confirmation if it needs to run a terminal command or install a tool – just approve when you’re comfortable.
- (Note: If anyone uses the Cursor editor, it has a similar agentic mode built-in. In Cursor, you just prompt via its chat, and it can auto-apply changes; there’s even an “auto-run” mode for fully automatic execution.)

## How Agent Mode Differs from Copilot Completions and ChatGPT

- Copilot Completions (standard): These are the inline suggestions as you type – Copilot predicts the next line or block of code based on context ￼. It’s passive and acts like an autocomplete on steroids, but it won’t execute code or make multi-file edits on its own.
- ChatGPT (regular): A general AI chatbot interface. While you can paste code and ask questions, it isn’t integrated with your editor or aware of your whole codebase. It won’t modify your files directly – you’d have to copy its suggestions into your code manually. It’s great for explanations or small snippets, but it’s outside your development environment.
- Agent Mode (Copilot Chat or Cursor): An integrated AI agent that can take on multi-step coding tasks with minimal input. You give a directive, and it can edit multiple files, create new files, and even run commands to achieve the goal ￼. It’s like an AI pair-programmer that not only suggests but actually acts (under your supervision). The agent will iterate as needed – for instance, if tests fail, it can adjust the code and try again ￼.
- Interactive vs. Manual: With completions, you’re manually accepting suggestions; with ChatGPT, you’re copying answers; with agent mode, the AI is actively working in your workspace. Agent mode often requires you to oversee and approve certain actions (especially anything destructive or installing packages), but it handles the heavy lifting once you give the go-ahead.

## How It Works (Under the Hood) & How AI Tools Operate

- Autonomous workflow: When you prompt agent mode, the AI breaks the request into steps: editing code, running tests or tools, reading results, and continuing. It uses a combination of direct code edits and tool invocations to accomplish the task ￼. Essentially, it’s running a loop of plan → execute → check → adjust until the task is done or needs guidance.
- Context awareness: The agent reads your codebase context to find relevant sections to change. You don’t always have to specify file names – it will try to locate where changes are needed. (For example, “add a logout button” will make it search for UI and auth code automatically.)
- No true understanding: Remember, the AI doesn’t truly understand our code like a human; it predicts what to do based on training and context. It can’t feel uncertainty or know when it’s making a wrong assumption – it’s just following patterns. So if something is tricky or ambiguous, you have to explicitly tell it to double-check or consider alternatives (it won’t spontaneously second-guess itself).
- Supervision is key: Think of the agent as a diligent but literal assistant. It will do exactly (and only) what you ask. If the request is too general (“make this faster”), it might do something superficial. If the task is mis-specified, it might solve the wrong problem. So, we need to guide it and review its output. You are still in charge: the agent speeds things up, but you must validate the changes.
- Potential pitfalls (“reward hacking”): Because the AI aims to fulfill your request, it might take unintended shortcuts. For example, if you say “make all tests pass,” the naive agent might be tempted to simply disable failing tests or alter test assertions to pass – technically achieving “all tests green” but not in the way you intended. We have to be mindful how we phrase goals (focus on fixing the code, not just pleasing the metrics) and always inspect what it actually did. The bottom line: trust but verify.

## The context problem

- The latest generation of LLMs are able to handle more context than ever before. They can, at least on paper, "remember" hundreds of thousands of tokens of context. But there's a catch: the more context you give them, the more their performance is degraded — especially when it comes to tool use.
- The trick to getting agents to write working code is to narrowly tailor the context they are given to include lots of information about what they need to know, and as little of what they don't as possible.
- This can be as simple as starting lots of new chats for each task, rather than using the same long chat with an overflowing context window. But there are some other ways to help:
  - Rules
  - Modes
  - MCP

### Rules

- Rules are a way to control the agent's behavior. They are a list of custom instructions that are injected into the agent's context. For example, I use several rules control agents' behavior in Cursor.
  - [GitHub usage](../img/rules/GitHub%20usage.png), [TDD](../img/rules/TDD.png), [User rule](../img/rules/User%20rule.png)

## Getting the Most Out of Coding Agents (Tips & Best Practices)

- Be explicit with your prompts: Don’t just describe a problem – clearly tell the AI what you want it to do. For example, instead of “This API call isn’t working right.”, say “Investigate why this API call fails and fix the issue.” You can certainly explain context first (which is helpful), but include a directive. If you want an explanation or code review instead of an edit, ask for that specifically (e.g. “Explain what this function is doing and if there’s a bug in it”). No instruction often = unpredictable output.
- Communicate uncertainty or limits: If you’re unsure about a solution or want it to be cautious, mention that. For instance, “I’m not positive about the approach for X, let’s think it through,” or “If this change affects other modules, please let me know before proceeding.” The AI will otherwise charge ahead confidently. By stating uncertainties, you prompt the agent to double-check and not assume things.
- Take it step-by-step for big changes: For large or complex tasks, consider asking the agent to outline a plan before writing code. For example, “If adding this feature requires changes in multiple files, list what you would change first.” This way, you both agree on the approach. You can even have it implement in stages. This prevents the AI from making a bunch of changes blindly. Essentially, break complex requests into smaller chunks and iterate with the AI.
- Always verify and test the output: After the agent does its thing, review the diffs or code suggestions. Run your application or tests to see if everything truly works. The agent is pretty good at iterative fixes (it will often run tests and fix failures on its own), but you should still make sure the solution makes sense and fits requirements. Use the AI’s help to fix follow-up issues, but keep an eye out for any logic mistakes or omissions – don’t assume it’s 100% correct.
- Favor fixing root causes over band-aids: When debugging, guide the AI to address the underlying problem, not just silence the symptom. If a test failed, rather than saying “make the test pass” (which might encourage a cheat), say “find and fix the bug causing this test to fail.” Similarly, avoid instructions like “just get rid of that error” without context – be specific about what a proper fix would entail. This ensures the agent’s solution is meaningful and not a hack.
- Use it for grunt work: Free yourself from tedious coding chores. You can write a rough draft of code and ask the agent to clean it up or optimize it. For example, “Here’s some messy JSON parsing code, can you refactor it to be cleaner?” or “Add comments to this function” or “Fix the linter warnings in this file.” The agent excels at polish and repetitive tasks. This way, you do the creative part, and the AI handles the boilerplate or cleanup.
- Collaborative debugging: If you and the agent are stuck on a bug, try increasing visibility. Ask the agent to add detailed logging or even write a quick test to isolate the issue. For example, “Insert debug prints to show the values of X, Y, Z at each step of this loop,” or “Create a small test case for function A with scenario B.” The AI can help instrument the code, and then you both examine the output. This iterative back-and-forth can crack tough problems. Don’t hesitate to have the AI explain the bug once the extra info is available.
- Set rules/guardrails for safety: Most agent tools allow some control over what the AI can do. For instance, you can configure which terminal commands it’s allowed to run, or require confirmation every time. Take advantage of this! If you’re nervous about it installing packages or deleting files, set those restrictions. (In Cursor, for example, you have a command allow/deny list; in Copilot VS Code, you’ll get a prompt to confirm each tool/command by default – don’t just blindly hit yes if it seems odd.) These rules help prevent accidents while you gain trust in the agent.
- Choose the right mode for the task: Ask Mode (Q&A chat) is great for quick questions or getting explanations. Edit Mode (if available) is for single-file or focused changes where you want to approve each suggestion. Agent Mode is for letting the AI handle a broader task across the project. If you just need a small fix or answer, you don’t need full agent autonomy – stick to chat or edit. Save agent mode for when you actually want it to orchestrate a bigger change or multi-step solution.
- Consider custom modes or profiles: Some tools let you customize the AI’s behavior. For example, you might have a “strict mode” where the agent follows a style guide or certain rules, or a profile that emphasizes security best practices. If your team has specific coding standards, you can instruct the agent with those upfront (some Copilot Chat versions allow adding custom system instructions or using prompt files). It’s worth exploring these settings to make the AI align better with your needs.
- Know about MCP (Model Context Protocol): This is an advanced feature for power users. MCP allows the agent to connect with external tools and data sources to inform its actions ￼. In practical terms, an MCP “server” could let the AI do things like look up documentation, query a database, or perform domain-specific analyses during its run. When enabled, the agent can pull in outside info and adjust its strategy on the fly, making it even more autonomous and powerful ￼. For our talk, just be aware this exists – if someone wants to experiment, they could set up MCP integrations to expand what the agent can do. (It’s optional, and likely something to try after getting comfortable with the basics.)
- Stay in control (the human touch): Finally, remember that these AI tools are here to assist, not replace our thinking. Use them to boost productivity and handle rote work, but you direct the show. Feel free to intervene, ask the agent why it chose something, or override its decisions. The best results come from a collaboration: you provide insight and oversight, and the agent provides speed and convenience. Keep the tone fun and interactive when using it – treat it like a teammate – but always apply your own judgment before merging that PR or deploying that change.
